## ðŸ”€ [Pull Request #557](https://github.com/ikawrakow/ik_llama.cpp/pull/557) - CUDA: MMQ for iqX_r4 quants 

| **Author** | `ikawrakow` |
| :--- | :--- |
| **State** | ðŸ”€ **Merged** |
| **Source Branch** | `ik/cuda_iqk_r4` |
| **Target Branch** | `main` |
| **Created** | 2025-06-25 |
| **Updated** | 2025-06-26 |
| **Merged** | 2025-06-26 |

---

## ðŸ“„ Description

CUDA matrix multiplications for `IQ2_K_R4, ..., IQ5_K_R4` quants on the main branch are implemented via deqantize to `fp16` (or `bf16`) + cuBLAS. As a result, there is a constant overhead for the dequantization step, which leads to relatively low performance when the number of tokens being processed is small. This is often the case for MoE models with many experts where each expert "sees" a small fraction of the tokens. For instance, for DeepSeek-R1/V3, for a batch size of 4096 tokens, experts will process on average just 128 tokens.

This PR addresses the issue by adding quantized matrix multiplication kernels (a.k.a., MMQ) for `IQ2_K_R4, IQ3_K_R4, IQ4_K_R4, IQ5_K_R4`. 

The benefit is illustrated with the following graph, which shows prompt processing performance as a function of prompt length for LlaMA-3.1-8B-Instruct using pure `IQ2_K_R4` quantization. GPU is RTX-4080. Black circles are for the main branch, red circles for this PR. While working on the PR I made the interesting observation that for these quants (all have block size of 16 weights, so use the much less efficient MMQ kernel template), dequantize+cuBLAS becomes faster than MMQ for batch sizes greater than 1000 tokens or so. Hence in the PR MMQ gets used for batches of fewer than 1024 tokens. The blue circles show MMQ-only. At 128 tokens, the new MMQ implementation is two times faster than dequantize+cuBLAS, so I expect to see a positive impact on prompt processing speed for @ubergarm's `*_R4` DeepSeek models.

![iqk](https://github.com/user-attachments/assets/0068aab7-fcc9-498f-b93c-c2a9759abd19)

---

## ðŸ’¬ Conversation

ðŸ‘¤ **ubergarm** commented on **2025-06-25** at **15:39:08**

Ran one test of my `IQ2_K_R4` on the Thread Ripper Pro 24x core offloading some layers onto 2x RTX A6000 GPUs showing some uplift for PP with this PR. I didn't try larger batch sizes at it sounds like this mostly benefits smaller batch sizes. Also I could have offloaded a couple more layers at least which would likely help given this boosts the CUDA code path speeds.

<details>

<summary>ðŸ‘ˆ sweep-bench command, data, and screen-shot of nvtop</summary>

I had some VRAM left so could proably have taken another layer or two each GPU.

![pr557-nvtop-screenshot](https://github.com/user-attachments/assets/a2568709-abb2-4e03-acfe-7d59920b2dfe)

```bash
model=DeepSeek-R1-0528-IQ2_K_R4-00001-of-00005.gguf
./build/bin/llama-sweep-bench \
    --model "$model" \
    --no-mmap \
    --ctx-size 8704 \
    -ctk q8_0 \
    -mla 3 -fa \
    -fmoe \
    -amb 512 \
    -ngl 99 \
    -ot "blk\.(3|4|5|6|7|8|9|10|11|12)\.ffn_.*=CUDA0" \
    -ot "blk\.(13|14|15|16|17|18|19|20|21|22)\.ffn_.*=CUDA1" \
    -ot exps=CPU \
    --warmup-batch \
    --threads 24
```

## PR557@b3417c93
|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|   512 |    128 |      0 |    3.891 |   131.57 |    8.242 |    15.53 |
|   512 |    128 |    512 |    4.628 |   110.62 |    8.311 |    15.40 |
|   512 |    128 |   1024 |    4.355 |   117.56 |    8.197 |    15.62 |
|   512 |    128 |   1536 |    4.240 |   120.76 |    8.299 |    15.42 |
|   512 |    128 |   2048 |    4.268 |   119.97 |    8.253 |    15.51 |
|   512 |    128 |   2560 |    4.660 |   109.88 |    8.490 |    15.08 |
|   512 |    128 |   3072 |    4.418 |   115.89 |    8.573 |    14.93 |
|   512 |    128 |   3584 |    4.550 |   112.52 |    8.517 |    15.03 |
|   512 |    128 |   4096 |    5.525 |    92.67 |    8.552 |    14.97 |
|   512 |    128 |   4608 |    4.770 |   107.33 |    8.485 |    15.09 |
|   512 |    128 |   5120 |    4.931 |   103.84 |    8.585 |    14.91 |
|   512 |    128 |   5632 |    4.901 |   104.47 |    8.975 |    14.26 |
|   512 |    128 |   6144 |    5.039 |   101.61 |    8.812 |    14.53 |
|   512 |    128 |   6656 |    5.124 |    99.93 |    8.901 |    14.38 |
|   512 |    128 |   7168 |    5.119 |   100.02 |    8.961 |    14.28 |
|   512 |    128 |   7680 |    5.200 |    98.46 |    8.836 |    14.49 |
|   512 |    128 |   8192 |    5.363 |    95.46 |    9.309 |    13.75 |

## main@b5f2f001
|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|   512 |    128 |      0 |    4.348 |   117.76 |    8.091 |    15.82 |
|   512 |    128 |    512 |    4.418 |   115.89 |    8.195 |    15.62 |
|   512 |    128 |   1024 |    4.520 |   113.27 |    8.200 |    15.61 |
|   512 |    128 |   1536 |    4.695 |   109.06 |    8.220 |    15.57 |
|   512 |    128 |   2048 |    4.787 |   106.96 |    8.258 |    15.50 |
|   512 |    128 |   2560 |    4.871 |   105.11 |    8.389 |    15.26 |
|   512 |    128 |   3072 |    4.960 |   103.23 |    8.453 |    15.14 |
|   512 |    128 |   3584 |    5.034 |   101.71 |    8.466 |    15.12 |
|   512 |    128 |   4096 |    5.152 |    99.37 |    8.448 |    15.15 |
|   512 |    128 |   4608 |    5.352 |    95.66 |    8.502 |    15.06 |
|   512 |    128 |   5120 |    5.423 |    94.41 |    8.523 |    15.02 |
|   512 |    128 |   5632 |    5.505 |    93.01 |    8.732 |    14.66 |
|   512 |    128 |   6144 |    5.490 |    93.27 |    8.706 |    14.70 |
|   512 |    128 |   6656 |    5.479 |    93.45 |    8.826 |    14.50 |
|   512 |    128 |   7168 |    5.595 |    91.51 |    8.783 |    14.57 |
|   512 |    128 |   7680 |    5.656 |    90.52 |    8.835 |    14.49 |
|   512 |    128 |   8192 |    5.800 |    88.28 |    8.985 |    14.25 |

</details>

![sweep-bench-pr557](https://github.com/user-attachments/assets/052420c8-caf9-412a-aa36-b636183334e7)

---

ðŸ‘¤ **ikawrakow** commented on **2025-06-25** at **16:04:37**

Thanks! Interesting that the performance fluctuations increase with this PR. There is also no reason TG performance to change.

---

ðŸ‘¤ **ubergarm** commented on **2025-06-25** at **17:02:56**

> Thanks! Interesting that the performance fluctuations increase with this PR. There is also no reason TG performance to change.

Yeah I was wondering about those two dips myself. I will try to get another run in to see as the rig was doing some downloading in the background which could account for some variability in the TG.

My larger sized quants e.g. IQ3_K_R4 are actually using the IQ4_KS_R4 for ffn_down etc.. I had chosen the `KS` quants over `K` as the larger 32 block size seems to give faster speeds. So this PR should be best for my smallest IQ2_K_R4 as there are no `KS` quants available for those smaller bpw's.

Also thanks I'm always impressed with your creativity in optimizing some of the quants we've put out there! Really appreciate it!