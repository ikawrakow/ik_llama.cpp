## ðŸ”€ [Pull Request #132](https://github.com/ikawrakow/ik_llama.cpp/pull/132) - Q5_K_R4

| **Author** | `ikawrakow` |
| :--- | :--- |
| **State** | ðŸ”€ **Merged** |
| **Source Branch** | `ik/q5_k_r4` |
| **Target Branch** | `main` |
| **Created** | 2024-12-10 |
| **Updated** | 2024-12-10 |
| **Merged** | 2024-12-10 |

---

## ðŸ“„ Description

Follow up of [#118](https://github.com/ikawrakow/ik_llama.cpp/issues/118), [#119](https://github.com/ikawrakow/ik_llama.cpp/issues/119), [#120](https://github.com/ikawrakow/ik_llama.cpp/issues/120), [#121](https://github.com/ikawrakow/ik_llama.cpp/issues/121), [#122](https://github.com/ikawrakow/ik_llama.cpp/issues/122), [#123](https://github.com/ikawrakow/ik_llama.cpp/issues/123), [#129](https://github.com/ikawrakow/ik_llama.cpp/issues/129), [#130](https://github.com/ikawrakow/ik_llama.cpp/issues/130)  for `Q5_K`. 

We get a large speedup on `ARM_NEON` and non-negligible gains on `AVX2/Zen4`.  Here is `PP-512` for LLaMA-3.1-8B on `Zen4` (Ryzen-7950X), `ARM_NEON` (M2-Max) and `AVX2` (Ryzen-5975WX)

| Platform |  Threads | Q5_K | Q5_K_R4 | Speedup |
| ---: | ---: | ---: | ---: | ---: |
| ARM_NEON |  8 |  61.07 Â± 0.95  | 96.13 Â± 2.38  | 1.574 |
| Zen4            | 16 | 188.73 Â± 0.75   | 248.30 Â± 0.29  | 1.316 |
| AVX2           | 32 | 188.11 Â± 0.29 |  269.18 Â± 0.40  | 1.431 |

On `AVX2/Zen4` we gain even for TG. Here results for TG-128 on LLaMA-3.1-8B with different numbers of threads:

| Platform |  Threads | Q6_K | Q6_K_R4 | Speedup |
| ---: | ---: | ---: | ---: | ---: |
| Zen4            | 1 |  5.12 Â± 0.00   | 7.07 Â± 0.01  |  1.380 |
|                      | 2 |  9.31 Â± 0.00 | 11.54 Â± 0.0  |  1.240 |
|                      | 4 |  11.33 Â± 0.37  | 11.89 Â± 0.00  |  1.049 |
| AVX2           | 2 | 4.04 Â± 0.00    | 6.40 Â± 0.00  | 1.584 |
|                     | 4 | 7.57 Â± 0.00    | 9.95 Â± 0.00  | 1.314 |
|                     | 8 |  9.75 Â± 0.00  | 11.00 Â± 0.00  | 1.128 |

I decided to check the current state of mainline `llama.cpp` for `Q5_K_S`.

Hahaha - here is what we get on my M2-Max (`build: 7736837d (4274)`)

| model                          |       size |     params | backend    | threads |          test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       8 |         pp512 |         27.69 Â± 0.09 |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       2 |         tg128 |          6.39 Â± 0.01 | 
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       4 |         tg128 |         12.18 Â± 0.02 |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       8 |         tg128 |         19.68 Â± 0.64 |

The performance gap in prompt processing for `Q5_K` has now grown to 3.5X, and it is ~30% slower for TG with 2 threads.

Here is what I get on my Ryzen-7950X (`build: 26a8406b (4295)`)

| model                          |       size |     params | backend    | threads |          test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |      16 |         pp512 |         75.88 Â± 0.26 |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       1 |         tg128 |          4.10 Â± 0.00 |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       2 |         tg128 |          7.66 Â± 0.01 |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       4 |         tg128 |         11.26 Â± 0.00 |
| llama 8B Q5_K - Small          |   5.21 GiB |     8.03 B | CPU        |       8 |         tg128 |         11.20 Â± 0.22 |

3.26X slower for prompt processing, 72%/51% slower for TG at 1/2 thread.