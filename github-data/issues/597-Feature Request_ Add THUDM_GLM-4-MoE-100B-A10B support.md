### âœ¨ [#597](https://github.com/ikawrakow/ik_llama.cpp/issues/597) - Feature Request: Add THUDM/GLM-4-MoE-100B-A10B support

| **Author** | `ubergarm` |
| :--- | :--- |
| **State** | âœ… **Open** |
| **Created** | 2025-07-10 |
| **Updated** | 2025-07-14 |

---

#### Description

The THUDM dev [zRzRzRzRzRzRzR](https://github.com/zRzRzRzRzRzRzR) seems to be adding support for a new yet unreleased `THUDM/GLM-4-MoE-100B-A10B` model architechture to vLLM currently [here](https://github.com/vllm-project/vllm/pull/20736/files#diff-c2cd72327248d1c1aa3d4b29ec9e47314d9893bfeff94e927841cd640fac84c1R351)

It is not confirmed, but this demo might be hosting the model currently: https://chat.z.ai/

Some more speculation on [r/LocalLLaMA here as well](https://www.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/).

If it looks promising, I might try to add support for this nice sized MoE when it is ready.

---

#### ðŸ’¬ Conversation

ðŸ‘¤ **arch-btw** commented the **2025-07-14** at **23:51:59**:<br>

Yes, I look forward to this release myself!

Just a heads up though, the name appears to be a placeholder:

<img width="705" height="318" alt="Image" src="https://github.com/user-attachments/assets/871f3c9c-6b93-424b-8265-77c2dd18426f" />

From [here](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking/discussions/6#6871d6dde775c2dbf1c756c5).