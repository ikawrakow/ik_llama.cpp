## ğŸ“Œ [Issue #452](https://github.com/ikawrakow/ik_llama.cpp/issues/452) - Falcon H1 Support

| **Author** | `Downtown-Case` |
| :--- | :--- |
| **State** | âŒ **Closed** |
| **Created** | 2025-05-23 |
| **Updated** | 2025-06-27 |
| **Labels** | `enhancement` |

---

## ğŸ“„ Description

A hybrid transformers/mamba2 series with good performance: https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df

Officially supported via their fork of llama.cpp here: https://github.com/tiiuae/llama.cpp-Falcon-H1

Support for ik_llama.cpp's tighter quantization schemes would be nice :). Maybe something in this fork can shrink the Mamba2 context cache as well?

---

## ğŸ’¬ Conversation

ğŸ‘¤ **ikawrakow** commented on **2025-05-24** at **07:04:24**

Have you though about adding a feature request to the llama.cpp-Falcon-H1 authors?

---

ğŸ‘¤ **Downtown-Case** commented on **2025-06-02** at **18:19:21**

Seems their implementation needs more time in the oven anyway.

---

ğŸ‘¤ **Downtown-Case** commented on **2025-06-27** at **14:31:42**

Closing this